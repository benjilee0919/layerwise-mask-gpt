# Installation Guide for Layer-wise Mask Scheduling in GPT-2

## 1. System Requirements

### Hardware
- CPU: Modern multi-core processor
- RAM: 16 GB (32 GB recommended)
- GPU (recommended): NVIDIA GPU with CUDA support  
  *All experiments in this project were run on an NVIDIA A100 (Colab).*

### Software
- OS: Linux, macOS, or Windows
- Python: 3.8–3.11
- Git: For cloning the repository

---

## 2. Create Python Environment

### Option A: Conda (recommended)

```bash
conda create -n lms-gpt python=3.10
conda activate lms-gpt
```

### Option B: Python venv

```bash
python -m venv lms-gpt-env
# Windows
lms-gpt-env\Scripts\activate
# macOS/Linux
source lms-gpt-env/bin/activate
```

---

## 3. Install Dependencies

### GPU Users (CUDA-enabled)

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

### CPU‑Only Users

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
```

---

## 4. Clone Repository

```bash
git clone <repository-url>
cd layerwise-mask-gpt
```

If Git is unavailable, download the ZIP version of the project and extract it manually.

---

## 5. Dataset Handling

TinyStories is automatically downloaded by the training script.  
No manual dataset download is required.

Optional manual download:

```bash
python -c "from src.dataset import prepare_dataset; prepare_dataset('tinystories')"
```

---

## 6. Running Experiments

### Baseline GPT‑2 (full attention)

```bash
python src/train_baseline.py \
  --dataset tiny_stories \
  --config config/train_config.json \
  --output_dir models/baseline_gpt2
```

### LMS Model (Layer‑wise Mask Scheduling)

```bash
python src/train_masked.py \
  --dataset tiny_stories \
  --config config/train_config.json \
  --schedule config/schedule_config.json \
  --output_dir models/mask_gpt2
```

Trained metrics and logs will be saved under `results/`.

---

## 7. Reproducing Plots

Use the notebooks under:

```
notebooks/
  02_baseline_eval.ipynb
  03_mask_schedule_eval.ipynb
```

These generate evaluation loss comparisons, speed plots, and training curves.

---

## 8. Troubleshooting

### CUDA not detected

```bash
nvidia-smi
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### GPU Out‑of‑Memory

Adjust `train_config.json`:

```json
{
  "batch_size": 4,
  "gradient_accumulation_steps": 4
}
```

### Import errors

```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

---

## 9. Verification

```bash
python -c "
import torch, transformers
print('✓ PyTorch:', torch.__version__)
print('✓ Transformers:', transformers.__version__)
print('CUDA available:', torch.cuda.is_available())
"
```

---

This concludes the installation instructions for the Layer‑wise Mask Scheduling GPT‑2 project.