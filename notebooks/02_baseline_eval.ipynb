{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "import time\n",
    "from src.dataset import prepare_dataset, get_dataloader\n",
    "from src.evaluate import ModelEvaluator, load_model\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c2554",
   "metadata": {},
   "source": [
    "## 1. Load Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline models (assuming they've been trained)\n",
    "baseline_models = {}\n",
    "model_paths = {\n",
    "    'baseline_tiny': '../models/baseline_gpt2_tiny',\n",
    "    'baseline_wiki': '../models/baseline_gpt2_wiki'\n",
    "}\n",
    "\n",
    "# Try to load models, or use pretrained GPT-2 if not available\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        print(f\"Loading {name} from {path}...\")\n",
    "        model, tokenizer = load_model(path, 'baseline')\n",
    "        baseline_models[name] = {'model': model, 'tokenizer': tokenizer, 'path': path}\n",
    "        print(f\"âœ“ Loaded {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not load {name}: {e}\")\n",
    "        print(f\"Using pretrained GPT-2 instead...\")\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        baseline_models[name] = {'model': model, 'tokenizer': tokenizer, 'path': 'gpt2'}\n",
    "        print(f\"âœ“ Using pretrained GPT-2 for {name}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(baseline_models)} baseline models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9ced7",
   "metadata": {},
   "source": [
    "## 2. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ba6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "for name, model_info in baseline_models.items():\n",
    "    model = model_info['model']\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = total_params * 4 / 1024**2  # Assuming float32\n",
    "    \n",
    "    print(f\"=== {name.upper()} ===\")\n",
    "    print(f\"Model path: {model_info['path']}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: {model_size_mb:.1f} MB\")\n",
    "    print(f\"Config: {model.config}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd6eb5",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe273519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test datasets\n",
    "print(\"Preparing test datasets...\")\n",
    "\n",
    "# TinyStories dataset\n",
    "tiny_datasets, tiny_tokenizer = prepare_dataset('tiny_stories', max_length=512)\n",
    "tiny_test_loader = get_dataloader(tiny_datasets['test'], batch_size=8, shuffle=False)\n",
    "\n",
    "# WikiText-103 dataset  \n",
    "wiki_datasets, wiki_tokenizer = prepare_dataset('wikitext-103', max_length=1024)\n",
    "wiki_test_loader = get_dataloader(wiki_datasets['test'], batch_size=4, shuffle=False)\n",
    "\n",
    "test_datasets = {\n",
    "    'tiny_stories': tiny_test_loader,\n",
    "    'wikitext': wiki_test_loader\n",
    "}\n",
    "\n",
    "print(f\"âœ“ Prepared test datasets\")\n",
    "print(f\"  TinyStories test: {len(tiny_datasets['test'])} examples\")\n",
    "print(f\"  WikiText test: {len(wiki_datasets['test'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fd571",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on both datasets\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model_info in baseline_models.items():\n",
    "    model = model_info['model']\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EVALUATING: {model_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    evaluator = ModelEvaluator(model, tokenizer, device)\n",
    "    evaluation_results[model_name] = {}\n",
    "    \n",
    "    # Evaluate on both datasets\n",
    "    for dataset_name, test_loader in test_datasets.items():\n",
    "        print(f\"\\n--- Evaluating on {dataset_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Full evaluation\n",
    "            results = evaluator.full_evaluation(test_loader)\n",
    "            evaluation_results[model_name][dataset_name] = results\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"Perplexity: {results['perplexity']:.2f}\")\n",
    "            print(f\"Throughput: {results['throughput']['tokens_per_second']:.1f} tokens/sec\")\n",
    "            print(f\"Latency (mean): {results['latency']['length_512']['mean_latency_ms']:.1f} ms\")\n",
    "            \n",
    "            if 'gpu_memory_allocated_mb' in results['memory']:\n",
    "                print(f\"GPU Memory: {results['memory']['gpu_memory_allocated_mb']:.1f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name} on {dataset_name}: {e}\")\n",
    "            evaluation_results[model_name][dataset_name] = None\n",
    "\n",
    "print(\"\\nâœ“ Model evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9ba92",
   "metadata": {},
   "source": [
    "## 5. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e23840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for visualization\n",
    "def extract_metrics(results):\n",
    "    metrics = {\n",
    "        'model': [],\n",
    "        'dataset': [],\n",
    "        'perplexity': [],\n",
    "        'throughput': [],\n",
    "        'latency': [],\n",
    "        'memory': []\n",
    "    }\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        for dataset_name, dataset_results in model_results.items():\n",
    "            if dataset_results is not None:\n",
    "                metrics['model'].append(model_name)\n",
    "                metrics['dataset'].append(dataset_name)\n",
    "                metrics['perplexity'].append(dataset_results['perplexity'])\n",
    "                metrics['throughput'].append(dataset_results['throughput']['tokens_per_second'])\n",
    "                metrics['latency'].append(dataset_results['latency']['length_512']['mean_latency_ms'])\n",
    "                metrics['memory'].append(dataset_results['memory'].get('gpu_memory_allocated_mb', 0))\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "metrics_df = extract_metrics(evaluation_results)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation results\n",
    "if not metrics_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Perplexity comparison\n",
    "    sns.barplot(data=metrics_df, x='dataset', y='perplexity', hue='model', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Perplexity Comparison')\n",
    "    axes[0, 0].set_ylabel('Perplexity (lower is better)')\n",
    "    \n",
    "    # Throughput comparison\n",
    "    sns.barplot(data=metrics_df, x='dataset', y='throughput', hue='model', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Throughput Comparison')\n",
    "    axes[0, 1].set_ylabel('Tokens/Second (higher is better)')\n",
    "    \n",
    "    # Latency comparison\n",
    "    sns.barplot(data=metrics_df, x='dataset', y='latency', hue='model', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Latency Comparison')\n",
    "    axes[1, 0].set_ylabel('Latency ms (lower is better)')\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    if metrics_df['memory'].sum() > 0:  # Only plot if we have GPU memory data\n",
    "        sns.barplot(data=metrics_df, x='dataset', y='memory', hue='model', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('GPU Memory Usage')\n",
    "        axes[1, 1].set_ylabel('Memory MB')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'GPU Memory\\nData Not Available', \n",
    "                       transform=axes[1, 1].transAxes, ha='center', va='center')\n",
    "        axes[1, 1].set_title('GPU Memory Usage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/baseline_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No evaluation results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ed331",
   "metadata": {},
   "source": [
    "## 6. Text Generation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aff5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample texts to analyze quality\n",
    "def generate_sample_texts(model, tokenizer, prompts, max_length=100, num_samples=3):\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            print(f\"\\nPrompt: '{prompt}'\")\n",
    "            print(\"Generated texts:\")\n",
    "            \n",
    "            # Encode prompt\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                # Generate\n",
    "                output = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=input_ids.shape[1] + max_length,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                # Decode\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                generated_texts.append(generated_text)\n",
    "                \n",
    "                print(f\"  {i+1}: {generated_text}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Define prompts for different datasets\n",
    "prompts = {\n",
    "    'tiny_stories': [\n",
    "        \"Once upon a time, there was a\",\n",
    "        \"The little girl wanted to\",\n",
    "        \"In the forest, they found\"\n",
    "    ],\n",
    "    'general': [\n",
    "        \"The theory of relativity\",\n",
    "        \"Machine learning is\",\n",
    "        \"Climate change affects\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate samples for each model\n",
    "for model_name, model_info in baseline_models.items():\n",
    "    model = model_info['model'].to(device)\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEXT GENERATION: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Choose appropriate prompts\n",
    "    if 'tiny' in model_name:\n",
    "        test_prompts = prompts['tiny_stories']\n",
    "    else:\n",
    "        test_prompts = prompts['general']\n",
    "    \n",
    "    generated_texts = generate_sample_texts(\n",
    "        model, tokenizer, test_prompts, max_length=50, num_samples=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493e385",
   "metadata": {},
   "source": [
    "## 7. Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention patterns in baseline model\n",
    "def extract_attention_patterns(model, tokenizer, text, layer_idx=11):\n",
    "    \"\"\"Extract attention patterns from a specific layer.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    \n",
    "    # Forward pass with attention output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_attentions=True)\n",
    "        attentions = outputs.attentions[layer_idx]  # Shape: [batch, heads, seq_len, seq_len]\n",
    "    \n",
    "    return attentions.cpu().numpy(), inputs\n",
    "\n",
    "# Visualize attention patterns\n",
    "def visualize_attention(attention, tokens, layer_idx, head_idx=0):\n",
    "    \"\"\"Visualize attention pattern for a specific head.\"\"\"\n",
    "    attention_head = attention[0, head_idx]  # [seq_len, seq_len]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention_head,\n",
    "        xticklabels=[tokenizer.decode([token]) for token in tokens],\n",
    "        yticklabels=[tokenizer.decode([token]) for token in tokens],\n",
    "        cmap='Blues',\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(f'Attention Pattern - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Key Tokens')\n",
    "    plt.ylabel('Query Tokens')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "# Analyze attention patterns for one model\n",
    "if baseline_models:\n",
    "    model_name = list(baseline_models.keys())[0]\n",
    "    model_info = baseline_models[model_name]\n",
    "    model = model_info['model'].to(device)\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    \n",
    "    sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    \n",
    "    print(f\"Analyzing attention patterns for {model_name}...\")\n",
    "    print(f\"Sample text: '{sample_text}'\")\n",
    "    \n",
    "    # Extract attention from last layer\n",
    "    attention, inputs = extract_attention_patterns(model, tokenizer, sample_text, layer_idx=-1)\n",
    "    tokens = inputs['input_ids'][0].cpu().numpy()\n",
    "    \n",
    "    print(f\"Attention shape: {attention.shape}\")\n",
    "    print(f\"Tokens: {[tokenizer.decode([token]) for token in tokens]}\")\n",
    "    \n",
    "    # Visualize attention for first head\n",
    "    fig = visualize_attention(attention, tokens, layer_idx=-1, head_idx=0)\n",
    "    plt.savefig('../results/figures/baseline_attention_pattern.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate attention statistics\n",
    "    attention_stats = {\n",
    "        'mean_attention': np.mean(attention),\n",
    "        'max_attention': np.max(attention),\n",
    "        'attention_entropy': -np.sum(attention * np.log(attention + 1e-10), axis=-1).mean(),\n",
    "        'attention_sparsity': np.mean(attention < 0.01)  # Fraction of near-zero weights\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAttention Statistics:\")\n",
    "    for key, value in attention_stats.items():\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3c515",
   "metadata": {},
   "source": [
    "## 8. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ad45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not metrics_df.empty:\n",
    "    summary_stats = metrics_df.groupby('dataset').agg({\n",
    "        'perplexity': ['mean', 'std'],\n",
    "        'throughput': ['mean', 'std'],\n",
    "        'latency': ['mean', 'std'],\n",
    "        'memory': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nPerformance by Dataset:\")\n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Best performing model per metric\n",
    "    print(\"\\nBest Models per Metric:\")\n",
    "    \n",
    "    for dataset in metrics_df['dataset'].unique():\n",
    "        dataset_df = metrics_df[metrics_df['dataset'] == dataset]\n",
    "        print(f\"\\n{dataset.upper()}:\")\n",
    "        \n",
    "        # Best perplexity (lowest)\n",
    "        best_ppl = dataset_df.loc[dataset_df['perplexity'].idxmin()]\n",
    "        print(f\"  Best Perplexity: {best_ppl['model']} ({best_ppl['perplexity']:.2f})\")\n",
    "        \n",
    "        # Best throughput (highest)\n",
    "        best_throughput = dataset_df.loc[dataset_df['throughput'].idxmax()]\n",
    "        print(f\"  Best Throughput: {best_throughput['model']} ({best_throughput['throughput']:.1f} tokens/sec)\")\n",
    "        \n",
    "        # Best latency (lowest)\n",
    "        best_latency = dataset_df.loc[dataset_df['latency'].idxmin()]\n",
    "        print(f\"  Best Latency: {best_latency['model']} ({best_latency['latency']:.1f} ms)\")\n",
    "else:\n",
    "    print(\"No evaluation results available for summary\")\n",
    "\n",
    "# Save all results\n",
    "results_summary = {\n",
    "    'evaluation_results': evaluation_results,\n",
    "    'metrics_summary': metrics_df.to_dict() if not metrics_df.empty else {},\n",
    "    'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('../results/baseline/comprehensive_evaluation.json', 'w') as f:\n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    import json\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return super().default(obj)\n",
    "    \n",
    "    json.dump(results_summary, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "print(\"\\nâœ“ Evaluation results saved to ../results/baseline/comprehensive_evaluation.json\")\n",
    "print(\"\\nðŸŽ¯ Baseline evaluation completed! Use these results as reference for masked model comparison.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
