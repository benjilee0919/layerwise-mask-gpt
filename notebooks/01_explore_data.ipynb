{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c30aa5",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading WikiText-103...\")\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-103-v1\", cache_dir=\"../data/processed\")\n",
    "\n",
    "print(\"Loading TinyStories...\")\n",
    "tinystories = load_dataset(\"roneneldan/TinyStories\", cache_dir=\"../data/processed\")\n",
    "\n",
    "print(\"Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset statistics\n",
    "def get_dataset_stats(dataset, name):\n",
    "    stats = {}\n",
    "    \n",
    "    for split in dataset.keys():\n",
    "        texts = dataset[split]['text']\n",
    "        \n",
    "        # Filter out empty texts\n",
    "        texts = [text for text in texts if text.strip()]\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats[split] = {\n",
    "            'num_examples': len(texts),\n",
    "            'total_chars': sum(len(text) for text in texts),\n",
    "            'avg_chars_per_example': np.mean([len(text) for text in texts]),\n",
    "            'median_chars_per_example': np.median([len(text) for text in texts]),\n",
    "            'total_words': sum(len(text.split()) for text in texts),\n",
    "            'avg_words_per_example': np.mean([len(text.split()) for text in texts])\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "wikitext_stats = get_dataset_stats(wikitext, \"WikiText-103\")\n",
    "tinystories_stats = get_dataset_stats(tinystories, \"TinyStories\")\n",
    "\n",
    "print(\"=== WikiText-103 Statistics ===\")\n",
    "for split, stats in wikitext_stats.items():\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"  Examples: {stats['num_examples']:,}\")\n",
    "    print(f\"  Total characters: {stats['total_chars']:,}\")\n",
    "    print(f\"  Avg chars/example: {stats['avg_chars_per_example']:.1f}\")\n",
    "    print(f\"  Total words: {stats['total_words']:,}\")\n",
    "    print(f\"  Avg words/example: {stats['avg_words_per_example']:.1f}\")\n",
    "\n",
    "print(\"\\n=== TinyStories Statistics ===\")\n",
    "for split, stats in tinystories_stats.items():\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"  Examples: {stats['num_examples']:,}\")\n",
    "    print(f\"  Total characters: {stats['total_chars']:,}\")\n",
    "    print(f\"  Avg chars/example: {stats['avg_chars_per_example']:.1f}\")\n",
    "    print(f\"  Total words: {stats['total_words']:,}\")\n",
    "    print(f\"  Avg words/example: {stats['avg_words_per_example']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8906c6",
   "metadata": {},
   "source": [
    "## 2. Sample Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e552a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample texts\n",
    "print(\"=== WikiText-103 Samples ===\")\n",
    "wikitext_samples = [text for text in wikitext['train']['text'][:1000] if text.strip()]\n",
    "\n",
    "for i, sample in enumerate(wikitext_samples[:3]):\n",
    "    print(f\"\\nSample {i+1} ({len(sample)} chars, {len(sample.split())} words):\")\n",
    "    print(sample[:500] + \"...\" if len(sample) > 500 else sample)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n\\n=== TinyStories Samples ===\")\n",
    "tinystories_samples = [text for text in tinystories['train']['text'][:1000] if text.strip()]\n",
    "\n",
    "for i, sample in enumerate(tinystories_samples[:3]):\n",
    "    print(f\"\\nSample {i+1} ({len(sample)} chars, {len(sample.split())} words):\")\n",
    "    print(sample[:500] + \"...\" if len(sample) > 500 else sample)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28989634",
   "metadata": {},
   "source": [
    "## 3. Text Length Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ef0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text length distributions\n",
    "def analyze_length_distribution(dataset, name, max_samples=10000):\n",
    "    # Get sample of texts\n",
    "    texts = [text for text in dataset['train']['text'][:max_samples] if text.strip()]\n",
    "    \n",
    "    # Calculate lengths\n",
    "    char_lengths = [len(text) for text in texts]\n",
    "    word_lengths = [len(text.split()) for text in texts]\n",
    "    \n",
    "    # Statistics\n",
    "    char_stats = {\n",
    "        'min': np.min(char_lengths),\n",
    "        'max': np.max(char_lengths),\n",
    "        'mean': np.mean(char_lengths),\n",
    "        'median': np.median(char_lengths),\n",
    "        'std': np.std(char_lengths),\n",
    "        'q25': np.percentile(char_lengths, 25),\n",
    "        'q75': np.percentile(char_lengths, 75),\n",
    "        'q95': np.percentile(char_lengths, 95),\n",
    "        'q99': np.percentile(char_lengths, 99)\n",
    "    }\n",
    "    \n",
    "    word_stats = {\n",
    "        'min': np.min(word_lengths),\n",
    "        'max': np.max(word_lengths),\n",
    "        'mean': np.mean(word_lengths),\n",
    "        'median': np.median(word_lengths),\n",
    "        'std': np.std(word_lengths),\n",
    "        'q25': np.percentile(word_lengths, 25),\n",
    "        'q75': np.percentile(word_lengths, 75),\n",
    "        'q95': np.percentile(word_lengths, 95),\n",
    "        'q99': np.percentile(word_lengths, 99)\n",
    "    }\n",
    "    \n",
    "    return char_lengths, word_lengths, char_stats, word_stats\n",
    "\n",
    "# Analyze both datasets\n",
    "wiki_char_lens, wiki_word_lens, wiki_char_stats, wiki_word_stats = analyze_length_distribution(wikitext, \"WikiText-103\")\n",
    "tiny_char_lens, tiny_word_lens, tiny_char_stats, tiny_word_stats = analyze_length_distribution(tinystories, \"TinyStories\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"=== Length Distribution Statistics ===\")\n",
    "print(\"\\nWikiText-103 Character Lengths:\")\n",
    "for key, value in wiki_char_stats.items():\n",
    "    print(f\"  {key}: {value:.1f}\")\n",
    "\n",
    "print(\"\\nTinyStories Character Lengths:\")\n",
    "for key, value in tiny_char_stats.items():\n",
    "    print(f\"  {key}: {value:.1f}\")\n",
    "\n",
    "print(\"\\nWikiText-103 Word Lengths:\")\n",
    "for key, value in wiki_word_stats.items():\n",
    "    print(f\"  {key}: {value:.1f}\")\n",
    "\n",
    "print(\"\\nTinyStories Word Lengths:\")\n",
    "for key, value in tiny_word_stats.items():\n",
    "    print(f\"  {key}: {value:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ff015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Character length distributions\n",
    "axes[0, 0].hist(wiki_char_lens, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[0, 0].hist(tiny_char_lens, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[0, 0].set_title('Character Length Distribution')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlim(0, 2000)  # Zoom in for better visibility\n",
    "\n",
    "# Word length distributions\n",
    "axes[0, 1].hist(wiki_word_lens, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[0, 1].hist(tiny_word_lens, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[0, 1].set_title('Word Length Distribution')\n",
    "axes[0, 1].set_xlabel('Words')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlim(0, 400)  # Zoom in for better visibility\n",
    "\n",
    "# Log scale character distributions\n",
    "axes[1, 0].hist(wiki_char_lens, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[1, 0].hist(tiny_char_lens, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[1, 0].set_title('Character Length Distribution (Log Scale)')\n",
    "axes[1, 0].set_xlabel('Characters')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Log scale word distributions\n",
    "axes[1, 1].hist(wiki_word_lens, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[1, 1].hist(tiny_word_lens, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[1, 1].set_title('Word Length Distribution (Log Scale)')\n",
    "axes[1, 1].set_xlabel('Words')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/dataset_length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7e038",
   "metadata": {},
   "source": [
    "## 4. Tokenization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tokenization for both datasets\n",
    "def analyze_tokenization(texts, tokenizer, name, max_samples=1000):\n",
    "    print(f\"\\n=== Tokenization Analysis: {name} ===\")\n",
    "    \n",
    "    # Sample texts for analysis\n",
    "    sample_texts = [text for text in texts[:max_samples] if text.strip()]\n",
    "    \n",
    "    token_lengths = []\n",
    "    compression_ratios = []\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        # Tokenize\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        num_tokens = len(tokens)\n",
    "        num_chars = len(text)\n",
    "        compression_ratio = num_chars / num_tokens if num_tokens > 0 else 0\n",
    "        \n",
    "        token_lengths.append(num_tokens)\n",
    "        compression_ratios.append(compression_ratio)\n",
    "    \n",
    "    # Statistics\n",
    "    token_stats = {\n",
    "        'mean': np.mean(token_lengths),\n",
    "        'median': np.median(token_lengths),\n",
    "        'std': np.std(token_lengths),\n",
    "        'min': np.min(token_lengths),\n",
    "        'max': np.max(token_lengths),\n",
    "        'q95': np.percentile(token_lengths, 95),\n",
    "        'q99': np.percentile(token_lengths, 99)\n",
    "    }\n",
    "    \n",
    "    compression_stats = {\n",
    "        'mean': np.mean(compression_ratios),\n",
    "        'median': np.median(compression_ratios),\n",
    "        'std': np.std(compression_ratios)\n",
    "    }\n",
    "    \n",
    "    print(f\"Token Length Statistics:\")\n",
    "    for key, value in token_stats.items():\n",
    "        print(f\"  {key}: {value:.1f}\")\n",
    "    \n",
    "    print(f\"\\nCompression Ratio (chars/token):\")\n",
    "    for key, value in compression_stats.items():\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    \n",
    "    # Estimate how many examples would fit in different sequence lengths\n",
    "    seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "    print(f\"\\nSequence Length Coverage:\")\n",
    "    for seq_len in seq_lengths:\n",
    "        coverage = np.mean(np.array(token_lengths) <= seq_len) * 100\n",
    "        print(f\"  {seq_len} tokens: {coverage:.1f}% of examples\")\n",
    "    \n",
    "    return token_lengths, compression_ratios, token_stats\n",
    "\n",
    "# Analyze tokenization\n",
    "wiki_tokens, wiki_compression, wiki_token_stats = analyze_tokenization(\n",
    "    wikitext['train']['text'], tokenizer, \"WikiText-103\"\n",
    ")\n",
    "\n",
    "tiny_tokens, tiny_compression, tiny_token_stats = analyze_tokenization(\n",
    "    tinystories['train']['text'], tokenizer, \"TinyStories\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d86a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token length distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Token length distributions\n",
    "axes[0, 0].hist(wiki_tokens, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[0, 0].hist(tiny_tokens, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[0, 0].set_title('Token Length Distribution')\n",
    "axes[0, 0].set_xlabel('Number of Tokens')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlim(0, 1000)  # Zoom in\n",
    "\n",
    "# Compression ratio distributions\n",
    "axes[0, 1].hist(wiki_compression, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[0, 1].hist(tiny_compression, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[0, 1].set_title('Compression Ratio Distribution (chars/token)')\n",
    "axes[0, 1].set_xlabel('Characters per Token')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Sequence length coverage\n",
    "seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "wiki_coverage = [np.mean(np.array(wiki_tokens) <= seq_len) * 100 for seq_len in seq_lengths]\n",
    "tiny_coverage = [np.mean(np.array(tiny_tokens) <= seq_len) * 100 for seq_len in seq_lengths]\n",
    "\n",
    "x = np.arange(len(seq_lengths))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, wiki_coverage, width, label='WikiText-103', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, tiny_coverage, width, label='TinyStories', alpha=0.8)\n",
    "axes[1, 0].set_title('Sequence Length Coverage')\n",
    "axes[1, 0].set_xlabel('Maximum Sequence Length')\n",
    "axes[1, 0].set_ylabel('Coverage (%)')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(seq_lengths)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylim(0, 100)\n",
    "\n",
    "# Sample tokenization examples\n",
    "axes[1, 1].axis('off')\n",
    "sample_text = \"Once upon a time, there was a little girl named Alice.\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "example_text = f\"Sample tokenization:\\n\\nOriginal: {sample_text}\\n\\n\"\n",
    "example_text += f\"Tokens ({len(tokens)}): {tokens}\\n\\n\"\n",
    "example_text += \"Decoded tokens:\\n\"\n",
    "for i, (token, decoded) in enumerate(zip(tokens, decoded_tokens)):\n",
    "    example_text += f\"{i:2d}: {token:5d} -> '{decoded}'\\n\"\n",
    "\n",
    "axes[1, 1].text(0.1, 0.9, example_text, transform=axes[1, 1].transAxes, \n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/tokenization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880c5e9",
   "metadata": {},
   "source": [
    "## 5. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary usage\n",
    "def analyze_vocabulary_usage(texts, tokenizer, name, max_samples=5000):\n",
    "    print(f\"\\n=== Vocabulary Analysis: {name} ===\")\n",
    "    \n",
    "    # Collect all tokens\n",
    "    all_tokens = []\n",
    "    sample_texts = [text for text in texts[:max_samples] if text.strip()]\n",
    "    \n",
    "    for text in sample_texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Statistics\n",
    "    total_tokens = len(all_tokens)\n",
    "    unique_tokens = len(token_counts)\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Unique tokens: {unique_tokens:,}\")\n",
    "    print(f\"Vocabulary coverage: {unique_tokens/tokenizer.vocab_size*100:.1f}%\")\n",
    "    print(f\"Type-token ratio: {unique_tokens/total_tokens:.4f}\")\n",
    "    \n",
    "    # Most common tokens\n",
    "    print(f\"\\nMost common tokens:\")\n",
    "    for token, count in token_counts.most_common(10):\n",
    "        decoded = tokenizer.decode([token]).replace('\\n', '\\\\n').replace(' ', '·')\n",
    "        print(f\"  {token:5d}: '{decoded:15s}' ({count:,} times, {count/total_tokens*100:.2f}%)\")\n",
    "    \n",
    "    # Least common tokens\n",
    "    print(f\"\\nLeast common tokens:\")\n",
    "    for token, count in list(token_counts.most_common())[-10:]:\n",
    "        decoded = tokenizer.decode([token]).replace('\\n', '\\\\n').replace(' ', '·')\n",
    "        print(f\"  {token:5d}: '{decoded:15s}' ({count} times)\")\n",
    "    \n",
    "    return token_counts, total_tokens, unique_tokens\n",
    "\n",
    "# Analyze vocabulary for both datasets\n",
    "wiki_vocab, wiki_total, wiki_unique = analyze_vocabulary_usage(\n",
    "    wikitext['train']['text'], tokenizer, \"WikiText-103\"\n",
    ")\n",
    "\n",
    "tiny_vocab, tiny_total, tiny_unique = analyze_vocabulary_usage(\n",
    "    tinystories['train']['text'], tokenizer, \"TinyStories\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea265bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vocabulary statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Token frequency distributions (log scale)\n",
    "wiki_freqs = list(wiki_vocab.values())\n",
    "tiny_freqs = list(tiny_vocab.values())\n",
    "\n",
    "axes[0, 0].hist(wiki_freqs, bins=50, alpha=0.7, label='WikiText-103', density=True)\n",
    "axes[0, 0].hist(tiny_freqs, bins=50, alpha=0.7, label='TinyStories', density=True)\n",
    "axes[0, 0].set_title('Token Frequency Distribution')\n",
    "axes[0, 0].set_xlabel('Token Frequency')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Zipf's law visualization\n",
    "wiki_sorted = sorted(wiki_freqs, reverse=True)\n",
    "tiny_sorted = sorted(tiny_freqs, reverse=True)\n",
    "\n",
    "ranks = np.arange(1, len(wiki_sorted) + 1)\n",
    "axes[0, 1].loglog(ranks, wiki_sorted, 'o-', alpha=0.7, label='WikiText-103', markersize=2)\n",
    "\n",
    "ranks = np.arange(1, len(tiny_sorted) + 1) \n",
    "axes[0, 1].loglog(ranks, tiny_sorted, 'o-', alpha=0.7, label='TinyStories', markersize=2)\n",
    "\n",
    "axes[0, 1].set_title(\"Zipf's Law: Token Rank vs Frequency\")\n",
    "axes[0, 1].set_xlabel('Rank')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Vocabulary coverage comparison\n",
    "datasets = ['WikiText-103', 'TinyStories']\n",
    "total_tokens = [wiki_total, tiny_total]\n",
    "unique_tokens = [wiki_unique, tiny_unique]\n",
    "coverage = [wiki_unique/tokenizer.vocab_size*100, tiny_unique/tokenizer.vocab_size*100]\n",
    "\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 0].bar(x - width, [t/1000 for t in total_tokens], width, label='Total Tokens (K)', alpha=0.8)\n",
    "axes[1, 0].bar(x, unique_tokens, width, label='Unique Tokens', alpha=0.8)\n",
    "axes[1, 0].bar(x + width, coverage, width, label='Vocab Coverage (%)', alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_title('Vocabulary Statistics Comparison')\n",
    "axes[1, 0].set_xlabel('Dataset')\n",
    "axes[1, 0].set_ylabel('Count / Percentage')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(datasets)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Summary statistics table\n",
    "axes[1, 1].axis('off')\n",
    "summary_data = [\n",
    "    ['Metric', 'WikiText-103', 'TinyStories'],\n",
    "    ['Total tokens', f'{wiki_total:,}', f'{tiny_total:,}'],\n",
    "    ['Unique tokens', f'{wiki_unique:,}', f'{tiny_unique:,}'],\n",
    "    ['Vocab coverage', f'{wiki_unique/tokenizer.vocab_size*100:.1f}%', \n",
    "     f'{tiny_unique/tokenizer.vocab_size*100:.1f}%'],\n",
    "    ['Type-token ratio', f'{wiki_unique/wiki_total:.4f}', f'{tiny_unique/tiny_total:.4f}'],\n",
    "    ['Avg token length', f'{wiki_token_stats[\"mean\"]:.1f}', f'{tiny_token_stats[\"mean\"]:.1f}'],\n",
    "    ['95th percentile', f'{wiki_token_stats[\"q95\"]:.1f}', f'{tiny_token_stats[\"q95\"]:.1f}']\n",
    "]\n",
    "\n",
    "table_text = \"\\n\".join([f\"{row[0]:<20} {row[1]:<15} {row[2]:<15}\" for row in summary_data])\n",
    "axes[1, 1].text(0.1, 0.9, table_text, transform=axes[1, 1].transAxes,\n",
    "                fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "axes[1, 1].set_title('Dataset Comparison Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/vocabulary_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37afbd0",
   "metadata": {},
   "source": [
    "## 6. Dataset Recommendations for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide recommendations based on analysis\n",
    "print(\"=== Dataset Recommendations for Training ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. SEQUENCE LENGTH:\")\n",
    "print(f\"   - For WikiText-103: Use max_length=1024 (covers ~{np.mean(np.array(wiki_tokens) <= 1024)*100:.1f}% of examples)\")\n",
    "print(f\"   - For TinyStories: Use max_length=512 (covers ~{np.mean(np.array(tiny_tokens) <= 512)*100:.1f}% of examples)\")\n",
    "print()\n",
    "\n",
    "print(\"2. BATCH SIZE RECOMMENDATIONS:\")\n",
    "print(\"   - WikiText-103: Start with smaller batches (8-16) due to longer sequences\")\n",
    "print(\"   - TinyStories: Can use larger batches (16-32) due to shorter sequences\")\n",
    "print()\n",
    "\n",
    "print(\"3. DATASET CHARACTERISTICS:\")\n",
    "print(\"   - WikiText-103: More diverse vocabulary, longer contexts, encyclopedia-style text\")\n",
    "print(\"   - TinyStories: Simpler vocabulary, shorter contexts, narrative-style text\")\n",
    "print()\n",
    "\n",
    "print(\"4. MASKING STRATEGY IMPLICATIONS:\")\n",
    "print(\"   - WikiText-103: Aggressive masking may hurt performance due to long-range dependencies\")\n",
    "print(\"   - TinyStories: More tolerant to attention masking due to simpler structure\")\n",
    "print()\n",
    "\n",
    "print(\"5. RECOMMENDED TRAINING ORDER:\")\n",
    "print(\"   1. Start with TinyStories for quick experimentation and hyperparameter tuning\")\n",
    "print(\"   2. Test masking schedules on TinyStories first\")\n",
    "print(\"   3. Apply best configurations to WikiText-103 for final evaluation\")\n",
    "print()\n",
    "\n",
    "# Save analysis results\n",
    "analysis_results = {\n",
    "    'wikitext_stats': {\n",
    "        'char_stats': wiki_char_stats,\n",
    "        'word_stats': wiki_word_stats,\n",
    "        'token_stats': wiki_token_stats,\n",
    "        'vocab_coverage': wiki_unique/tokenizer.vocab_size*100,\n",
    "        'type_token_ratio': wiki_unique/wiki_total\n",
    "    },\n",
    "    'tinystories_stats': {\n",
    "        'char_stats': tiny_char_stats,\n",
    "        'word_stats': tiny_word_stats,\n",
    "        'token_stats': tiny_token_stats,\n",
    "        'vocab_coverage': tiny_unique/tokenizer.vocab_size*100,\n",
    "        'type_token_ratio': tiny_unique/tiny_total\n",
    "    },\n",
    "    'recommendations': {\n",
    "        'wikitext_max_length': 1024,\n",
    "        'tinystories_max_length': 512,\n",
    "        'wikitext_batch_size': [8, 16],\n",
    "        'tinystories_batch_size': [16, 32]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/dataset_analysis.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(\"Analysis results saved to ../data/dataset_analysis.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
