{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe367e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from src.dataset import prepare_dataset, get_dataloader\n",
    "from src.evaluate import ModelEvaluator, load_model, compare_models\n",
    "from src.model.gpt2_custom import MaskedGPT2LMHeadModel\n",
    "from src.model.schedule import load_mask_schedule, create_predefined_schedules, visualize_schedule\n",
    "from src.model.mask_utils import (\n",
    "    create_layer_wise_masks, \n",
    "    get_attention_pattern_efficiency, \n",
    "    visualize_attention_pattern,\n",
    "    compare_attention_patterns\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c0a0c",
   "metadata": {},
   "source": [
    "## 1. Load Masking Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba63c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display available masking schedules\n",
    "schedule_config_path = '../config/schedule_config.json'\n",
    "\n",
    "try:\n",
    "    with open(schedule_config_path, 'r') as f:\n",
    "        schedule_config = json.load(f)\n",
    "    print(\"âœ“ Loaded schedule configuration\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Schedule config not found, creating predefined schedules...\")\n",
    "    from src.model.schedule import save_predefined_schedules\n",
    "    save_predefined_schedules(schedule_config_path)\n",
    "    \n",
    "    with open(schedule_config_path, 'r') as f:\n",
    "        schedule_config = json.load(f)\n",
    "    print(\"âœ“ Created and loaded predefined schedules\")\n",
    "\n",
    "print(f\"\\nAvailable schedules: {list(schedule_config['schedules'].keys())}\")\n",
    "\n",
    "# Display schedule details\n",
    "for name, schedule in schedule_config['schedules'].items():\n",
    "    print(f\"\\n{name.upper()}: {schedule['description']}\")\n",
    "    \n",
    "    # Count masked layers\n",
    "    masked_layers = sum(1 for layer_config in schedule['layers'].values() \n",
    "                       if layer_config['mask_type'] != 'none')\n",
    "    total_layers = len(schedule['layers'])\n",
    "    \n",
    "    print(f\"  Masked layers: {masked_layers}/{total_layers}\")\n",
    "    \n",
    "    # Show layer-wise configuration\n",
    "    print(f\"  Layer configuration:\")\n",
    "    for layer_idx, layer_config in list(schedule['layers'].items())[:6]:  # Show first 6\n",
    "        mask_type = layer_config['mask_type']\n",
    "        window_size = layer_config.get('window_size', 'N/A')\n",
    "        print(f\"    Layer {layer_idx}: {mask_type} (window: {window_size})\")\n",
    "    \n",
    "    if total_layers > 6:\n",
    "        print(f\"    ... (showing 6/{total_layers} layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3fbe7",
   "metadata": {},
   "source": [
    "## 2. Visualize Masking Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab62352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize masks for different schedules\n",
    "seq_length = 256  # Use smaller size for visualization\n",
    "num_layers = 12\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "schedule_names = ['full', 'half', 'quarter', 'aggressive']\n",
    "\n",
    "for idx, schedule_name in enumerate(schedule_names):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    schedule_data = schedule_config['schedules'][schedule_name]\n",
    "    \n",
    "    # Create masks for all layers\n",
    "    masks = create_layer_wise_masks(seq_length, num_layers, schedule_data['layers'])\n",
    "    \n",
    "    # Calculate efficiency statistics\n",
    "    total_attended = 0\n",
    "    total_possible = 0\n",
    "    \n",
    "    for layer_idx, mask in masks.items():\n",
    "        efficiency = get_attention_pattern_efficiency(mask)\n",
    "        total_attended += efficiency['attended_positions']\n",
    "        total_possible += efficiency['total_positions']\n",
    "    \n",
    "    overall_sparsity = 1 - (total_attended / total_possible)\n",
    "    memory_reduction = total_possible / total_attended\n",
    "    \n",
    "    # Visualize last layer mask as representative\n",
    "    last_layer_mask = masks[num_layers - 1].cpu().numpy()\n",
    "    \n",
    "    im = axes[idx].imshow(last_layer_mask, cmap='Blues', aspect='equal')\n",
    "    axes[idx].set_title(f'{schedule_name.upper()}\\n'\n",
    "                       f'Sparsity: {overall_sparsity:.2f}, '\n",
    "                       f'Memory: {memory_reduction:.1f}x')\n",
    "    axes[idx].set_xlabel('Key Position')\n",
    "    axes[idx].set_ylabel('Query Position')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=axes[idx], fraction=0.046)\n",
    "\n",
    "plt.suptitle('Attention Masking Patterns (Last Layer)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/masking_schedules_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26576e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layer-wise progression visualization\n",
    "schedule_name = 'half'  # Focus on half schedule for detailed analysis\n",
    "schedule_data = schedule_config['schedules'][schedule_name]\n",
    "\n",
    "# Create masks for all layers\n",
    "masks = create_layer_wise_masks(128, num_layers, schedule_data['layers'])\n",
    "\n",
    "# Calculate sparsity progression across layers\n",
    "layer_sparsity = []\n",
    "layer_window_sizes = []\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    mask = masks[layer_idx]\n",
    "    efficiency = get_attention_pattern_efficiency(mask)\n",
    "    layer_sparsity.append(efficiency['sparsity'])\n",
    "    \n",
    "    # Get window size for this layer\n",
    "    layer_config = schedule_data['layers'][str(layer_idx)]\n",
    "    window_size = layer_config.get('window_size')\n",
    "    layer_window_sizes.append(window_size if window_size else 128)  # Use seq_length as default\n",
    "\n",
    "# Plot layer-wise statistics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Sparsity progression\n",
    "axes[0].plot(range(num_layers), layer_sparsity, 'o-', linewidth=2, markersize=6)\n",
    "axes[0].set_title(f'Sparsity Progression: {schedule_name.upper()}')\n",
    "axes[0].set_xlabel('Layer Index')\n",
    "axes[0].set_ylabel('Attention Sparsity')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Window size progression\n",
    "axes[1].semilogy(range(num_layers), layer_window_sizes, 'o-', linewidth=2, markersize=6, color='orange')\n",
    "axes[1].set_title(f'Window Size Progression: {schedule_name.upper()}')\n",
    "axes[1].set_xlabel('Layer Index')\n",
    "axes[1].set_ylabel('Window Size (log scale)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/layer_wise_progression.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Layer-wise sparsity for {schedule_name}:\")\n",
    "for i, sparsity in enumerate(layer_sparsity):\n",
    "    window_size = layer_window_sizes[i]\n",
    "    print(f\"  Layer {i:2d}: {sparsity:.3f} sparsity (window: {window_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c198ffc",
   "metadata": {},
   "source": [
    "## 3. Load Masked Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to load trained masked models\n",
    "masked_models = {}\n",
    "schedule_names = ['half', 'quarter', 'aggressive']  # Skip 'full' as it's equivalent to baseline\n",
    "\n",
    "for schedule_name in schedule_names:\n",
    "    model_path = f'../models/mask_gpt2/{schedule_name}'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading {schedule_name} model from {model_path}...\")\n",
    "        model, tokenizer = load_model(model_path, 'masked')\n",
    "        masked_models[schedule_name] = {\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'path': model_path,\n",
    "            'schedule': schedule_config['schedules'][schedule_name]\n",
    "        }\n",
    "        print(f\"âœ“ Loaded {schedule_name} model\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not load {schedule_name} model: {e}\")\n",
    "        print(f\"Creating model with {schedule_name} schedule for demonstration...\")\n",
    "        \n",
    "        # Create model with the schedule for demonstration\n",
    "        try:\n",
    "            from transformers import GPT2Config\n",
    "            config = GPT2Config.from_pretrained('gpt2')\n",
    "            schedule_data = schedule_config['schedules'][schedule_name]\n",
    "            \n",
    "            model = MaskedGPT2LMHeadModel.from_pretrained('gpt2', mask_schedule=schedule_data)\n",
    "            tokenizer = model.transformer.wte.weight.data.new_zeros(1).device\n",
    "            from transformers import GPT2Tokenizer\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            masked_models[schedule_name] = {\n",
    "                'model': model,\n",
    "                'tokenizer': tokenizer,\n",
    "                'path': 'gpt2',  # Base model\n",
    "                'schedule': schedule_data\n",
    "            }\n",
    "            print(f\"âœ“ Created {schedule_name} model for demonstration\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âœ— Failed to create {schedule_name} model: {e2}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(masked_models)} masked models\")\n",
    "for name in masked_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100bf6c",
   "metadata": {},
   "source": [
    "## 4. Model Information and Schedule Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e794cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information and mask schedule summaries\n",
    "for name, model_info in masked_models.items():\n",
    "    model = model_info['model']\n",
    "    schedule = model_info['schedule']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODEL: {name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Basic model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model path: {model_info['path']}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Schedule summary\n",
    "    print(f\"\\nMask Schedule: {schedule['description']}\")\n",
    "    \n",
    "    # Calculate schedule efficiency\n",
    "    try:\n",
    "        masks = create_layer_wise_masks(512, 12, schedule['layers'])\n",
    "        \n",
    "        total_attended = 0\n",
    "        total_possible = 0\n",
    "        masked_layers = 0\n",
    "        \n",
    "        for layer_idx, mask in masks.items():\n",
    "            efficiency = get_attention_pattern_efficiency(mask)\n",
    "            total_attended += efficiency['attended_positions']\n",
    "            total_possible += efficiency['total_positions']\n",
    "            \n",
    "            # Check if layer is masked\n",
    "            layer_config = schedule['layers'][str(layer_idx)]\n",
    "            if layer_config['mask_type'] != 'none':\n",
    "                masked_layers += 1\n",
    "        \n",
    "        overall_sparsity = 1 - (total_attended / total_possible)\n",
    "        memory_reduction = total_possible / total_attended\n",
    "        \n",
    "        print(f\"Masked layers: {masked_layers}/12\")\n",
    "        print(f\"Overall sparsity: {overall_sparsity:.3f}\")\n",
    "        print(f\"Memory reduction: {memory_reduction:.2f}x\")\n",
    "        \n",
    "        # Show some layer details\n",
    "        print(f\"\\nLayer details (first 6):\")\n",
    "        for layer_idx in range(min(6, len(masks))):\n",
    "            layer_config = schedule['layers'][str(layer_idx)]\n",
    "            mask = masks[layer_idx]\n",
    "            efficiency = get_attention_pattern_efficiency(mask)\n",
    "            \n",
    "            mask_type = layer_config['mask_type']\n",
    "            window_size = layer_config.get('window_size', 'N/A')\n",
    "            sparsity = efficiency['sparsity']\n",
    "            \n",
    "            print(f\"  Layer {layer_idx}: {mask_type:15s} (window: {str(window_size):>4s}, sparsity: {sparsity:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate schedule efficiency: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58932fde",
   "metadata": {},
   "source": [
    "## 5. Prepare Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d78257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test datasets for evaluation\n",
    "print(\"Preparing test datasets...\")\n",
    "\n",
    "# Use smaller datasets for faster evaluation in this notebook\n",
    "tiny_datasets, _ = prepare_dataset('tiny_stories', max_length=512)\n",
    "tiny_test_loader = get_dataloader(\n",
    "    tiny_datasets['test'], \n",
    "    batch_size=4,  # Smaller batch for faster processing\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Prepared TinyStories test dataset\")\n",
    "print(f\"  Test examples: {len(tiny_datasets['test'])}\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"  Max sequence length: 512\")\n",
    "\n",
    "# Optionally prepare WikiText for more comprehensive analysis\n",
    "try:\n",
    "    wiki_datasets, _ = prepare_dataset('wikitext-103', max_length=1024)\n",
    "    wiki_test_loader = get_dataloader(\n",
    "        wiki_datasets['test'][:100],  # Use subset for faster evaluation\n",
    "        batch_size=2,\n",
    "        shuffle=False\n",
    "    )\n",
    "    print(f\"âœ“ Prepared WikiText-103 test subset (100 examples)\")\n",
    "    test_datasets = {\n",
    "        'tiny_stories': tiny_test_loader,\n",
    "        'wikitext': wiki_test_loader\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"âš  Could not prepare WikiText dataset: {e}\")\n",
    "    print(\"Continuing with TinyStories only...\")\n",
    "    test_datasets = {\n",
    "        'tiny_stories': tiny_test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecf7f1",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d86257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate masked models\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model_info in masked_models.items():\n",
    "    model = model_info['model']\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EVALUATING: {model_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    evaluator = ModelEvaluator(model, tokenizer, device)\n",
    "    evaluation_results[model_name] = {}\n",
    "    \n",
    "    # Evaluate on available datasets\n",
    "    for dataset_name, test_loader in test_datasets.items():\n",
    "        print(f\"\\n--- Evaluating on {dataset_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Quick evaluation (fewer batches for notebook)\n",
    "            print(\"Calculating perplexity...\")\n",
    "            # Use subset of data for faster evaluation\n",
    "            perplexity = evaluator.calculate_perplexity(\n",
    "                iter(list(test_loader)[:20])  # Use first 20 batches\n",
    "            )\n",
    "            \n",
    "            print(\"Measuring throughput...\")\n",
    "            throughput = evaluator.measure_throughput(\n",
    "                test_loader, num_batches=10  # Use fewer batches\n",
    "            )\n",
    "            \n",
    "            print(\"Measuring latency...\")\n",
    "            latency = evaluator.measure_latency(\n",
    "                input_length=256, num_trials=20  # Fewer trials\n",
    "            )\n",
    "            \n",
    "            print(\"Measuring memory usage...\")\n",
    "            memory = evaluator.measure_memory_usage()\n",
    "            \n",
    "            # Store results\n",
    "            evaluation_results[model_name][dataset_name] = {\n",
    "                'perplexity': perplexity,\n",
    "                'throughput': throughput,\n",
    "                'latency': latency,\n",
    "                'memory': memory\n",
    "            }\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"Results:\")\n",
    "            print(f\"  Perplexity: {perplexity:.2f}\")\n",
    "            print(f\"  Throughput: {throughput['tokens_per_second']:.1f} tokens/sec\")\n",
    "            print(f\"  Latency: {latency['mean_latency_ms']:.1f} ms\")\n",
    "            if 'gpu_memory_allocated_mb' in memory:\n",
    "                print(f\"  GPU Memory: {memory['gpu_memory_allocated_mb']:.1f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name} on {dataset_name}: {e}\")\n",
    "            evaluation_results[model_name][dataset_name] = None\n",
    "\n",
    "print(\"\\nâœ“ Masked model evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90fea95",
   "metadata": {},
   "source": [
    "## 7. Load Baseline Results for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe04d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load baseline results for comparison\n",
    "baseline_results = {}\n",
    "\n",
    "try:\n",
    "    with open('../results/baseline/comprehensive_evaluation.json', 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "        baseline_results = baseline_data.get('evaluation_results', {})\n",
    "    print(\"âœ“ Loaded baseline evaluation results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  No baseline results found, will create comparison with current data only\")\n",
    "    \n",
    "    # Create placeholder baseline results using GPT-2\n",
    "    try:\n",
    "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "        \n",
    "        baseline_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        baseline_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        baseline_tokenizer.pad_token = baseline_tokenizer.eos_token\n",
    "        \n",
    "        print(\"Creating baseline comparison with pretrained GPT-2...\")\n",
    "        baseline_evaluator = ModelEvaluator(baseline_model, baseline_tokenizer, device)\n",
    "        \n",
    "        baseline_results['baseline_gpt2'] = {}\n",
    "        \n",
    "        for dataset_name, test_loader in test_datasets.items():\n",
    "            print(f\"Evaluating baseline on {dataset_name}...\")\n",
    "            \n",
    "            # Quick baseline evaluation\n",
    "            perplexity = baseline_evaluator.calculate_perplexity(\n",
    "                iter(list(test_loader)[:20])\n",
    "            )\n",
    "            throughput = baseline_evaluator.measure_throughput(\n",
    "                test_loader, num_batches=10\n",
    "            )\n",
    "            latency = baseline_evaluator.measure_latency(\n",
    "                input_length=256, num_trials=20\n",
    "            )\n",
    "            memory = baseline_evaluator.measure_memory_usage()\n",
    "            \n",
    "            baseline_results['baseline_gpt2'][dataset_name] = {\n",
    "                'perplexity': perplexity,\n",
    "                'throughput': throughput,\n",
    "                'latency': latency,\n",
    "                'memory': memory\n",
    "            }\n",
    "            \n",
    "            print(f\"  Baseline perplexity: {perplexity:.2f}\")\n",
    "        \n",
    "        print(\"âœ“ Created baseline comparison results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create baseline comparison: {e}\")\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**baseline_results, **evaluation_results}\n",
    "print(f\"\\nTotal models for comparison: {len(all_results)}\")\n",
    "for model_name in all_results.keys():\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a8d06",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "def extract_comparison_metrics(results):\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, model_results in results.items():\n",
    "        for dataset_name, dataset_results in model_results.items():\n",
    "            if dataset_results is not None:\n",
    "                row = {\n",
    "                    'model': model_name,\n",
    "                    'dataset': dataset_name,\n",
    "                    'perplexity': dataset_results['perplexity'],\n",
    "                    'throughput': dataset_results['throughput']['tokens_per_second'],\n",
    "                    'latency': dataset_results['latency']['mean_latency_ms'],\n",
    "                    'memory': dataset_results['memory'].get('gpu_memory_allocated_mb', 0),\n",
    "                    'model_type': 'baseline' if 'baseline' in model_name else 'masked'\n",
    "                }\n",
    "                comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "comparison_df = extract_comparison_metrics(all_results)\n",
    "print(\"Comparison Results:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate relative performance vs baseline\n",
    "if not comparison_df.empty and 'baseline_gpt2' in comparison_df['model'].values:\n",
    "    print(\"\\n=== Relative Performance vs Baseline ===\")\n",
    "    \n",
    "    for dataset in comparison_df['dataset'].unique():\n",
    "        dataset_df = comparison_df[comparison_df['dataset'] == dataset]\n",
    "        baseline_row = dataset_df[dataset_df['model'].str.contains('baseline')]\n",
    "        \n",
    "        if not baseline_row.empty:\n",
    "            baseline_ppl = baseline_row['perplexity'].iloc[0]\n",
    "            baseline_throughput = baseline_row['throughput'].iloc[0]\n",
    "            baseline_latency = baseline_row['latency'].iloc[0]\n",
    "            \n",
    "            print(f\"\\n{dataset.upper()}:\")\n",
    "            print(f\"{'Model':<15} {'PPL Ratio':<12} {'Speed Ratio':<12} {'Latency Ratio':<15}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for _, row in dataset_df.iterrows():\n",
    "                if 'baseline' not in row['model']:\n",
    "                    ppl_ratio = row['perplexity'] / baseline_ppl\n",
    "                    speed_ratio = row['throughput'] / baseline_throughput\n",
    "                    latency_ratio = row['latency'] / baseline_latency\n",
    "                    \n",
    "                    print(f\"{row['model']:<15} {ppl_ratio:<12.3f} {speed_ratio:<12.3f} {latency_ratio:<15.3f}\")\n",
    "                    \n",
    "                    # Interpretation\n",
    "                    if ppl_ratio < 1.1 and speed_ratio > 1.1:\n",
    "                        print(f\"  â†’ âœ“ Good trade-off: minimal quality loss, speed gain\")\n",
    "                    elif ppl_ratio > 1.5:\n",
    "                        print(f\"  â†’ âš  Significant quality degradation\")\n",
    "                    elif speed_ratio < 1.1:\n",
    "                        print(f\"  â†’ âš  No significant speed improvement\")\n",
    "                    else:\n",
    "                        print(f\"  â†’ ðŸ¤” Mixed results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bf610",
   "metadata": {},
   "source": [
    "## 9. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "if not comparison_df.empty:\n",
    "    # Set up the plotting\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Color palette for model types\n",
    "    palette = {'baseline': '#1f77b4', 'masked': '#ff7f0e'}\n",
    "    \n",
    "    # 1. Perplexity comparison\n",
    "    sns.barplot(data=comparison_df, x='dataset', y='perplexity', \n",
    "                hue='model_type', ax=axes[0], palette=palette)\n",
    "    axes[0].set_title('Perplexity Comparison')\n",
    "    axes[0].set_ylabel('Perplexity (lower is better)')\n",
    "    \n",
    "    # 2. Throughput comparison\n",
    "    sns.barplot(data=comparison_df, x='dataset', y='throughput', \n",
    "                hue='model_type', ax=axes[1], palette=palette)\n",
    "    axes[1].set_title('Throughput Comparison')\n",
    "    axes[1].set_ylabel('Tokens/Second (higher is better)')\n",
    "    \n",
    "    # 3. Latency comparison\n",
    "    sns.barplot(data=comparison_df, x='dataset', y='latency', \n",
    "                hue='model_type', ax=axes[2], palette=palette)\n",
    "    axes[2].set_title('Latency Comparison')\n",
    "    axes[2].set_ylabel('Latency ms (lower is better)')\n",
    "    \n",
    "    # 4. Memory usage\n",
    "    if comparison_df['memory'].sum() > 0:\n",
    "        sns.barplot(data=comparison_df, x='dataset', y='memory', \n",
    "                    hue='model_type', ax=axes[3], palette=palette)\n",
    "        axes[3].set_title('Memory Usage')\n",
    "        axes[3].set_ylabel('GPU Memory MB')\n",
    "    else:\n",
    "        axes[3].text(0.5, 0.5, 'GPU Memory\\nData Not Available', \n",
    "                    transform=axes[3].transAxes, ha='center', va='center')\n",
    "        axes[3].set_title('Memory Usage')\n",
    "    \n",
    "    # 5. Efficiency vs Quality scatter plot\n",
    "    if len(comparison_df) > 1:\n",
    "        # Calculate efficiency score (inverse of latency)\n",
    "        comparison_df['efficiency'] = 1000 / comparison_df['latency']  \n",
    "        \n",
    "        for dataset in comparison_df['dataset'].unique():\n",
    "            dataset_data = comparison_df[comparison_df['dataset'] == dataset]\n",
    "            \n",
    "            for model_type in dataset_data['model_type'].unique():\n",
    "                type_data = dataset_data[dataset_data['model_type'] == model_type]\n",
    "                axes[4].scatter(type_data['efficiency'], 1/type_data['perplexity'], \n",
    "                              label=f'{dataset}_{model_type}', alpha=0.7, s=100)\n",
    "        \n",
    "        axes[4].set_xlabel('Efficiency (1000/latency)')\n",
    "        axes[4].set_ylabel('Quality (1/perplexity)')\n",
    "        axes[4].set_title('Efficiency vs Quality Trade-off')\n",
    "        axes[4].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 6. Model comparison by schedule\n",
    "    masked_only = comparison_df[comparison_df['model_type'] == 'masked']\n",
    "    if not masked_only.empty:\n",
    "        sns.barplot(data=masked_only, x='model', y='perplexity', ax=axes[5])\n",
    "        axes[5].set_title('Masked Models Perplexity')\n",
    "        axes[5].set_ylabel('Perplexity')\n",
    "        axes[5].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/masked_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No comparison data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab3698",
   "metadata": {},
   "source": [
    "## 10. Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47313507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention patterns for one masked model\n",
    "if masked_models:\n",
    "    # Use the first available masked model\n",
    "    model_name = list(masked_models.keys())[0]\n",
    "    model_info = masked_models[model_name]\n",
    "    model = model_info['model'].to(device)\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    schedule = model_info['schedule']\n",
    "    \n",
    "    print(f\"Analyzing attention patterns for: {model_name}\")\n",
    "    \n",
    "    # Sample text for analysis\n",
    "    sample_text = \"Once upon a time, there was a brave little mouse who lived in a big house.\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(sample_text, return_tensors='pt', truncation=True, max_length=128)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    tokens = input_ids[0].cpu().numpy()\n",
    "    \n",
    "    print(f\"Sample text: '{sample_text}'\")\n",
    "    print(f\"Tokens ({len(tokens)}): {[tokenizer.decode([token]) for token in tokens]}\")\n",
    "    \n",
    "    # Extract attention from model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_ids, output_attentions=True)\n",
    "            all_attentions = outputs.attentions  # List of attention tensors\n",
    "            \n",
    "            print(f\"\\nExtracted attention from {len(all_attentions)} layers\")\n",
    "            \n",
    "            # Visualize attention patterns for different layers\n",
    "            layers_to_show = [0, 5, 11]  # First, middle, last\n",
    "            fig, axes = plt.subplots(1, len(layers_to_show), figsize=(15, 5))\n",
    "            \n",
    "            if len(layers_to_show) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, layer_idx in enumerate(layers_to_show):\n",
    "                if layer_idx < len(all_attentions):\n",
    "                    attention = all_attentions[layer_idx][0, 0].cpu().numpy()  # First head\n",
    "                    \n",
    "                    im = axes[i].imshow(attention, cmap='Blues', aspect='equal')\n",
    "                    axes[i].set_title(f'Layer {layer_idx}\\n{schedule[\"layers\"][str(layer_idx)][\"mask_type\"]}')\n",
    "                    axes[i].set_xlabel('Key Position')\n",
    "                    axes[i].set_ylabel('Query Position')\n",
    "                    \n",
    "                    plt.colorbar(im, ax=axes[i], fraction=0.046)\n",
    "            \n",
    "            plt.suptitle(f'Attention Patterns: {model_name.upper()}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'../results/figures/{model_name}_attention_patterns.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate attention statistics\n",
    "            print(\"\\nAttention Statistics by Layer:\")\n",
    "            print(f\"{'Layer':<6} {'Type':<15} {'Window':<8} {'Sparsity':<10} {'Entropy':<10}\")\n",
    "            print(\"-\" * 55)\n",
    "            \n",
    "            for layer_idx, attention_tensor in enumerate(all_attentions[:6]):  # Show first 6\n",
    "                attention = attention_tensor[0, 0].cpu().numpy()  # First head\n",
    "                \n",
    "                # Calculate metrics\n",
    "                sparsity = np.mean(attention < 0.01)\n",
    "                entropy = -np.sum(attention * np.log(attention + 1e-10), axis=-1).mean()\n",
    "                \n",
    "                # Get layer config\n",
    "                layer_config = schedule['layers'][str(layer_idx)]\n",
    "                mask_type = layer_config['mask_type']\n",
    "                window_size = layer_config.get('window_size', 'N/A')\n",
    "                \n",
    "                print(f\"{layer_idx:<6} {mask_type:<15} {str(window_size):<8} {sparsity:<10.3f} {entropy:<10.3f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract attention patterns: {e}\")\n",
    "            print(\"This might be because the model doesn't support attention output\")\n",
    "else:\n",
    "    print(\"No masked models available for attention analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb5cd5",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c12f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MASK SCHEDULE EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Schedule efficiency summary\n",
    "print(\"\\n1. MASKING SCHEDULE EFFICIENCY:\")\n",
    "schedule_efficiency = {}\n",
    "\n",
    "for schedule_name in ['full', 'half', 'quarter', 'aggressive']:\n",
    "    if schedule_name in schedule_config['schedules']:\n",
    "        schedule_data = schedule_config['schedules'][schedule_name]\n",
    "        \n",
    "        try:\n",
    "            masks = create_layer_wise_masks(512, 12, schedule_data['layers'])\n",
    "            \n",
    "            total_attended = sum(\n",
    "                get_attention_pattern_efficiency(mask)['attended_positions']\n",
    "                for mask in masks.values()\n",
    "            )\n",
    "            total_possible = sum(\n",
    "                get_attention_pattern_efficiency(mask)['total_positions']\n",
    "                for mask in masks.values()\n",
    "            )\n",
    "            \n",
    "            overall_sparsity = 1 - (total_attended / total_possible)\n",
    "            memory_reduction = total_possible / total_attended\n",
    "            \n",
    "            schedule_efficiency[schedule_name] = {\n",
    "                'sparsity': overall_sparsity,\n",
    "                'memory_reduction': memory_reduction\n",
    "            }\n",
    "            \n",
    "            print(f\"  {schedule_name.upper():<12}: {overall_sparsity:.3f} sparsity, {memory_reduction:.2f}x memory reduction\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  {schedule_name.upper():<12}: Could not calculate ({e})\")\n",
    "\n",
    "# Performance summary\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\n2. PERFORMANCE COMPARISON:\")\n",
    "    \n",
    "    # Find best performing masked model\n",
    "    masked_models_df = comparison_df[comparison_df['model_type'] == 'masked']\n",
    "    baseline_models_df = comparison_df[comparison_df['model_type'] == 'baseline']\n",
    "    \n",
    "    if not masked_models_df.empty and not baseline_models_df.empty:\n",
    "        for dataset in comparison_df['dataset'].unique():\n",
    "            print(f\"\\n  {dataset.upper()}:\")\n",
    "            \n",
    "            dataset_masked = masked_models_df[masked_models_df['dataset'] == dataset]\n",
    "            dataset_baseline = baseline_models_df[baseline_models_df['dataset'] == dataset]\n",
    "            \n",
    "            if not dataset_baseline.empty:\n",
    "                baseline_ppl = dataset_baseline['perplexity'].iloc[0]\n",
    "                baseline_speed = dataset_baseline['throughput'].iloc[0]\n",
    "                \n",
    "                print(f\"    Baseline perplexity: {baseline_ppl:.2f}\")\n",
    "                print(f\"    Baseline speed: {baseline_speed:.1f} tokens/sec\")\n",
    "                \n",
    "                if not dataset_masked.empty:\n",
    "                    best_quality = dataset_masked.loc[dataset_masked['perplexity'].idxmin()]\n",
    "                    best_speed = dataset_masked.loc[dataset_masked['throughput'].idxmax()]\n",
    "                    \n",
    "                    print(f\"    Best quality: {best_quality['model']} (PPL: {best_quality['perplexity']:.2f})\")\n",
    "                    print(f\"    Best speed: {best_speed['model']} ({best_speed['throughput']:.1f} tokens/sec)\")\n",
    "                    \n",
    "                    # Calculate improvements\n",
    "                    quality_degradation = (best_quality['perplexity'] / baseline_ppl - 1) * 100\n",
    "                    speed_improvement = (best_speed['throughput'] / baseline_speed - 1) * 100\n",
    "                    \n",
    "                    print(f\"    Quality change: {quality_degradation:+.1f}%\")\n",
    "                    print(f\"    Speed improvement: {speed_improvement:+.1f}%\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n3. RECOMMENDATIONS:\")\n",
    "\n",
    "recommendations = [\n",
    "    \"ðŸ“Š Data Collection: Run full training and evaluation on larger datasets\",\n",
    "    \"ðŸ”¬ Hyperparameter Tuning: Fine-tune learning rates and schedules for masked models\",\n",
    "    \"ðŸ“ Sequence Length Analysis: Test performance on different sequence lengths\",\n",
    "    \"ðŸŽ¯ Task-Specific Evaluation: Test on downstream tasks (e.g., text generation, QA)\",\n",
    "    \"âš–ï¸ Trade-off Analysis: Find optimal balance between efficiency and quality\",\n",
    "    \"ðŸ”„ Dynamic Masking: Explore adaptive masking based on input complexity\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "evaluation_summary = {\n",
    "    'schedule_efficiency': schedule_efficiency,\n",
    "    'evaluation_results': evaluation_results,\n",
    "    'comparison_data': comparison_df.to_dict() if not comparison_df.empty else {},\n",
    "    'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "with open('../results/mask_scheduled/comprehensive_evaluation.json', 'w') as f:\n",
    "    # Handle numpy types for JSON serialization\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return super().default(obj)\n",
    "    \n",
    "    json.dump(evaluation_summary, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "print(\"\\nâœ“ Comprehensive evaluation results saved to ../results/mask_scheduled/comprehensive_evaluation.json\")\n",
    "print(\"\\nðŸŽ¯ Mask schedule evaluation completed!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Train models with different schedules on full datasets\")\n",
    "print(\"- Run comprehensive evaluation with larger test sets\")\n",
    "print(\"- Generate paper figures and final analysis\")\n",
    "print(\"- Write up results and conclusions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
